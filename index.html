<!DOCTYPE html>
<html>
<head>

  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="Image as an IMU: Estimating Camera Motion from a Single Motion-Blurred Image"/>
  <meta property="og:title" content="Image as an IMU"/>
  <meta property="og:description" content="Image as an IMU: Estimating Camera Motion from a Single Motion-Blurred Image"/>
  <meta property="og:url" content="https://jerredchen.github.io/image-as-imu/"/>


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>Image as an IMU</title>
  <!-- <link rel="icon" type="image/x-icon" href="static/images/favicon.ico"> -->
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>

  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Image as an IMU: Estimating Camera Motion from a Single Motion-Blurred Image</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="https://jerredchen.github.io/" target="_blank">Jerred Chen</a><sup></sup>,</span>
                <span class="author-block">
                  <a href="https://www.ron-clark.com/" target="_blank">Ronald Clark</a><sup></sup></span>
                  </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block">University of Oxford<br>arXiv 2025</span>
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                      <span class="link-block">
                        <a href="https://arxiv.org/pdf/2503.17358.pdf" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>

                    <!-- Supplementary PDF link -->
                    <span class="link-block">
                      <a href="static/pdfs/supplementary.pdf" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Supplementary</span>
                    </a>
                  </span>

                  <!-- Github link -->
                  <span class="link-block">
                    <!-- <a href="https://github.com/YOUR REPO HERE" target="_blank" -->
                    <a target="_blank"
                    class="external-link button is-disabled is-rounded is-normal is-dark">
                    <!-- class="external-link button is-normal is-rounded is-dark"> -->
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code (Coming Soon)</span>
                  </a>
                </span>

                <!-- ArXiv abstract Link -->
                <span class="link-block">
                  <a href="https://arxiv.org/abs/2503.17358" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Teaser video-->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <video poster="" preload="auto" id="tree" playsinline autoplay muted loop width="100%">
        <!-- Your video here -->
        <source src="static/videos/teaser_web.mp4"
        type="video/mp4">
      </video>
      <h2 class="subtitle has-text-centered">
        Given a single motion-blurred image, we exploit the motion blur cues to predict the camera velocity at that instant <i> without performing any deblurring</i>.
      </h2>
    </div>
  </div>
</section>
<!-- End teaser video -->

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            In many robotics and VR/AR applications, fast camera motions cause a high level of motion blur, causing existing camera pose estimation methods to fail. In this work, we propose a novel framework that leverages motion blur as a rich cue for motion estimation rather than treating it as an unwanted artifact. Our approach works by predicting a dense motion flow field and a monocular depth map directly from a single motion-blurred image. We then recover the instantaneous camera velocity by solving a linear least squares problem under the small motion assumption. In essence, our method produces an IMU-like measurement that robustly captures fast and aggressive camera movements. To train our model, we construct a large-scale dataset with realistic synthetic motion blur derived from ScanNet++v2 and further refine our model by training end-to-end on real data using our fully differentiable pipeline. Extensive evaluations on real-world benchmarks demonstrate that our method achieves state-of-the-art angular and translational velocity estimates, outperforming current methods like MASt3R and COLMAP.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->

<!-- Evaluation Setup -->
<section class="hero is-small">
  <br/>
  <div class="container is-max-desktop">
    <h2 class="title is-3 is-centered has-text-centered">Motivation</h2>
      <video poster="" id="tree" playsinline autoplay controls muted loop width="100%">
        <!-- Your video here -->
        <source src="static/videos/motivation_web.mp4"
        type="video/mp4">
      </video>
      <div class="content has-text-justified">
        <p> 
          A commonly held assumption for camera motion is that each image represents a single instantaneous snapshot in time. 
          However, in reality the camera undergoes continuous motion across the exposure time, which can render motion blur under fast motions. 
          While this can be problematic for standard pose estimation methods, we aim to exploit the motion blur artifact as a rich source of information to help estimate the motion.
        </p>
      </div>
  </div>
  <br/>
  <br/>
</section>
<!-- End of evaluation setup -->


<!-- Evaluation Setup -->
<section class="hero is-small">
  <br/>
  <div class="container is-max-desktop">
    <h2 class="title is-3 has-text-centered">Evaluation Setup</h2>
      <video poster="" id="tree" playsinline autoplay muted loop width="100%">
        <!-- Your video here -->
        <source src="static/videos/third_person_demo_web.mp4"
        type="video/mp4">
      </video>
      <div class="content has-text-centered">
        <p> 
          We conduct all evaluations using recorded footage from an iPhone 13 Pro with the <a href="https://github.com/strayrobots/scanner" target="_blank">StrayScanner app</a> . 
          The app is slightly modified to obtain the exposure time from ARKit.
        </p>
      </div>
  </div>
  <br/>
  <br/>
</section>
<!-- End of evaluation setup -->


<!-- Video carousel -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container is-centered has-text-centered">
      <h2 class="title is-3">Experimental Results</h2>
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <div class="content has-text-justified">
            <p>
              We evaluate our method on real-world motion-blurred videos. While the baseline methods must use multiple frames to compute the velocity, our network only takes a <i>single frame</i> as input. 
              Because the true direction of a single motion-blurred image is ambiguous, we flip the velocity direction as necessary based on the photometric error between frames. 
              We directly treat the gyroscope readings as the angular velocity ground truth, and we approximate the translational velocity ground truth using the ARKit poses and framerate. 
              Note that the angular velocity axes are x-up, y-left, z-backwards (using the IMU convention) whereas the the translational velocity axes are x-right, y-down, z-forward (using OpenCV convention).
            </p>
          </div>
        </div>
      </div>
      <div class="playback-controls has-text-centered mb-3">
        <button class="button is-small is-rounded is-info" id="speed-toggle">
          <span>Playback Speed: 0.25x</span>
        </button>
      </div>
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item item-video1">
          <video poster="" id="video1" playsinline autoplay controls muted loop height="100%">
            <!-- Your video file here -->
            <source src="static/videos/demo-cr.mp4"
            type="video/mp4">
          </video>
        </div>
        <div class="item item-video2">
          <video poster="" id="video2" playsinline autoplay controls muted loop height="100%">
            <!-- Your video file here -->
            <source src="static/videos/demo-billiards.mp4"
            type="video/mp4">
          </video>
        </div>
        <div class="item item-video3">
          <video poster="" id="video3" playsinline autoplay controls muted loop height="100%">
            <!-- Your video file here -->
            <source src="static/videos/demo-office.mp4"
            type="video/mp4">
          </video>
        </div>
      </div>
    </div>
  </div>
  <script>
    document.addEventListener('DOMContentLoaded', function() {
      const speedToggle = document.getElementById('speed-toggle');
      const videos = document.querySelectorAll('#results-carousel video');
      const speeds = [0.25, 0.5, 1.0];
      let currentSpeedIndex = 0;
      
      // Set initial playback rate
      videos.forEach(video => {
        video.playbackRate = speeds[currentSpeedIndex];
      });
      
      speedToggle.addEventListener('click', function() {
        currentSpeedIndex = (currentSpeedIndex + 1) % speeds.length;
        const newSpeed = speeds[currentSpeedIndex];
        
        videos.forEach(video => {
          video.playbackRate = newSpeed;
        });
        
        speedToggle.querySelector('span').textContent = `Playback Speed: ${newSpeed}x`;
      });
    });
  </script>
</section>
<!-- End video carousel -->


<!-- Quantitative Results -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3 has-text-centered">Quantitative Results</h2>
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <div class="item">
             <img src="static/images/tables.png" alt="Table Results">
          </div>
        </div>
      </div>
    </div>
  </div>
  <br/>
</section>
<!-- End of quantitative results -->

<!-- Animation. -->
<div class="columns is-centered">
  <div class="column is-three-fifths">
    <h3 class="title is-4 has-text-centered">Frame-by-Frame Inference</h3>
    <div class="content is-centered has-text-centered">
      <p>
        Our network uses a single frame as input <i>in complete isolation from the rest of the video</i>. 
        Try the slider to see how our method recovers the camera motion at a particular frame!
      </p>
    </div>
    <div class="columns is-vcentered animation-panel">
      <div class="column animation-video-column">
        <div id="animation-image-wrapper">
          Loading...
        </div>
        <input class="slider is-fullwidth is-large is-info"
                id="animation-slider"
                step="1" min="0" max="174" value="0" type="range">
      </div>
    </div>
    <br/>

  </div>
</div>
<!--/ Animation. -->


<!-- Method -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3 has-text-centered">Method Overview</h2>
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <div class="item">
             <img src="static/images/blur_method.jpg" alt="Method Overview">
          </div>
          <div class="content has-text-justified">
            <p> 
              Given a single image, our method predicts a dense motion flow field and a monocular depth map. 
              We then recover the instantaneous camera velocity with linear least squares using the known exposure time and intrinsics. 
              To disambiguate the velocity direction in a video, we use the predicted motion to compute the photometric error between the current frame and the previous/next frames and flip the direction if necessary.
            </p>
          </div>
        </div>
      </div>
    </div>
  </div>
  <br/>
</section>
<!-- End of method -->

<!-- Runtime -->
<section class="hero is-small">
  <div class="container is-max-desktop">
    <h2 class="title is-3 has-text-centered">Runtime Comparison</h2>
      <video poster="" id="tree" playsinline autoplay muted loop width="100%">
        <!-- Your video here -->
        <source src="static/videos/runtime_web.mp4"
        type="video/mp4">
      </video>
      <div class="content has-text-centered">
        <p> 
          We show the runtimes of our method against the baselines on one of the real-world videos. All methods are run on an Nvidia RTX 3090. 
          Even including the direction disambiguation, our method is significantly faster and runs in real-time at 30 Hz. 
        </p>
      </div>
  </div>
</section>
<!-- End of runtime -->


<!-- Paper poster -->
<!-- <section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <h2 class="title">Poster</h2>

      <iframe  src="static/pdfs/sample.pdf" width="100%" height="550">
          </iframe>
        
      </div>
    </div>
  </section> -->
<!--End paper poster -->


<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>@article{chen2025_imageimu,
  title     = {{Image as an IMU}: Estimating Camera Motion from a Single Motion-Blurred Image},
  author    = {Chen, Jerred and Clark, Ronald},
  journal   = {arXiv preprint},
  year      = {2025}
}</code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
